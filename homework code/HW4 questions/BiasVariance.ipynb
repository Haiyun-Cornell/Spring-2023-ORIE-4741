{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW4-Q2. Plotting bias, variance and decision trees\n",
    "\n",
    "In this problem, we'll investigate the bias and the variance of two different estimators. We'll see (once again) that fitting the data more precisely is not always a good idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(4741)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)\n",
    "\n",
    "Suppose we have a sinusoid function $f(x)=10\\sin(x)$. Our dataset $\\mathcal D$ will consist of $n=7$ data points drawn from the following probabilistic model. For each data point $x_i$ we randomly draw uniformly in $[0,6]$ and observe a noisy $y_i=f(x_i)+\\epsilon_i$, where $\\epsilon_i$ is some noise drawn from a standard normal distribution $\\mathcal N(0,1)$.\n",
    "\n",
    "Generate a sample dataset from this distribution. Plot this dataset $\\mathcal D$ and the true function $f(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)\n",
    "\n",
    "Fit a linear model $l(x)=w_0+w_1x$ to $\\mathcal D$.\n",
    "Plot this linear model $l(x)$ together with $\\mathcal D$ and $f(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)\n",
    "\n",
    "Fit a cubic model $c(x)=w_0+w_1x+w_2x^2+w_3x^3$ to $\\mathcal D$.\n",
    "Plot this cubic model $c(x)$ together with $\\mathcal D$ and $f(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d)\n",
    "\n",
    "Repeat parts b) and c) for 1000 different randomly drawn sets $\\mathcal D$.\n",
    "\n",
    "Average the 1000 linear models you generated to get the average linear model $\\bar{l}(x)$. Plot $\\bar{l}(x)$ with $f(x)$.\n",
    "\n",
    "Generate the average cubic model $\\bar{c}(x)$ in the same way, and plot it together with $f(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e)\n",
    "\n",
    "Describe the squared bias graphically with respect to the two plots in part d). Compute the squared bias of $\\bar{l}(x)$ and $\\bar{c}(x)$. Which model has smaller squared bias?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.integrate as integrate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f)\n",
    "\n",
    "Compute the variance of $\\bar{l}(x)$ and $\\bar{c}(x)$. Which model has smaller variance? How do you interpret this? Which model has smaller overall error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g)\n",
    "\n",
    "How do you think your results would depend on the number of points in the data set $\\mathcal D$? Feel free to perform an experiment to check. How many points would you need before the opposite model (from your answer in the previous question) has smaller overall error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### h)\n",
    "We can also investigate the bias-variance tradeoff for trees.\n",
    "\n",
    "Suppose our dataset $\\mathcal D$ consists of $n=10$ data points drawn randomly from two moons and is balanced. Points are labeled 1 in one moon and 0 in the other moon. We set 0.05 to be the standard deviation of Gaussian noise in the data.\n",
    "\n",
    "Generate a sample dataset using random_state=2, and plot it. (Note: because the data is 2-dimensional, have the x- and y-coordinates on the axes and represent the label with the color blue for 1 and red for 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "###YOUR CODES###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a decision tree model with 2 splits to $\\mathcal D$. Use random_state= 2. (Hint: for n splits, there are at most n+1 leaf nodes.)\n",
    "\n",
    "Plot this tree in the form of rectangular partitions of the feature space with $\\mathcal D$. Use lines to mark the splits, and shade the regions with the correct label color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "###YOUR CODES###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i)\n",
    "\n",
    "Repeat part h) to fit models for 1000 different randomly drawn sets $\\mathcal D$ from a newly generated set of 10,000 points using random_state=2. Average the individual 2-split decision trees to get the \"average\" 2-split decision tree model $\\bar{t}(x)$ using the bagging ensemble method learned in class. Plot $\\bar{t}(x)$ for all 10,000 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "###YOUR CODES###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
